# -*- coding: utf-8 -*-
"""S2_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c_dYtvXChE3Rkm1scVVrqwfQzNOA0ni6

Word2Vec - wariant ze zliczaniem słów (skupiamy się na liczbie słów w otoczeniu a nie na podobieństwu znaczeń)

####***1.Import bibliotek***
"""

import pandas as pd #tworzenie df
import matplotlib.pyplot as plt #histogram
from sklearn.model_selection import train_test_split #podział zbioru danych
from sklearn.feature_extraction.text import CountVectorizer #zliczanie słów wektorowo
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras import activations
from keras.callbacks import EarlyStopping, ModelCheckpoint #callbacki
from sklearn.metrics import confusion_matrix #macierz pomyłek

"""####***2.Zbiór danych***"""

#pobieranie danych z repozytorium

!git clone https://bitbucket.org/pn2020/data-for-data-science.git #klonowanie repozytorium
!ls #katalog domyślny
!ls data-for-data-science/

#Tworzenie ramki danych

df=pd.read_csv("data-for-data-science/text_classification_sciences.csv", sep=";")

"""####***3.Przygotowanie danych***"""

#Przegląd danych

df.head()
df["science"].drop_duplicates()
plt.hist(df["science"])

"""dokumentacja scikit-learn -> https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"""

train_x, test_x, train_y, test_y=train_test_split(df["text"],df["science"]) #podział zbioru na uczący i testowy
plt.hist(train_y) #przegląd danych testowych

"""####***4.Zliczanie słów i wektoryzacja***
CountVectorizer służy do przekształcenia tekstu na wektory liczbowe reprezentujące częstość występowania słów w tekście.





"""

count_vect=CountVectorizer(max_features=1000) #tylko 1000 najczęściej występujących słów zostanie zastosowanych jako cechy wektora (długość wektora)
count_vect.fit(train_x)

"""Macierz rzadka - większość elementów jest równa zero; liczba elementów, które nie są zerami, jest zwykle znacznie mniejsza od liczby zer
Macierz gęsta - większość elementów jest niezerowa; liczba elementów, które nie są zerami, jest zwykle podobna do liczby zer.
"""

train_count=count_vect.transform(train_x)
test_count=count_vect.transform(test_x)
train_count #macierz rzadka (zakodowane tylko liczby które nie są zerami)
train_count.todense()[0,:] #pierwszy wiersz macierzy gęstej

count_vect.get_feature_names_out()[26] #sprawdzamy jakie słowo jest na 26 miejscu w wektorze
train_x.iloc[0] #wyświetla pierwszy wiersz macierzy
count_vect.get_feature_names_out() #unikalny zbiór słów w zbiorze danych

''''pd.get_dummies - zmieniamy kategorie na wektory binarne
(1 - należy do kategorii, 0 - nie należy)'''
train_labels_cat=np.array(pd.get_dummies(train_y))
test_labels_cat=np.array(pd.get_dummies(test_y))
train_labels_cat[:10] #przegląd 10 obserwacji

"""####***5. Budowa i uczenie sieci***

***Sequential()***  model sekwencyjny -> Każda warstwa przetwarza informacje otrzymane od poprzedniej warstwy,
a wyniki przetwarzania są przekazywane do następnej warstwy.

***softmax*** to funkcja aktywacji, która jest często stosowana jako ostatnia warstwa w sieci neuronowej do klasyfikacji wieloklasowej. Funkcja ta przekształca wektor wyjściowy z N elementami na wektor N-elementowy, którego każdy element odpowiada prawdopodobieństwu przynależności do jednej z N klas.

***categorical_crossentropy*** mierzy różnicę między prawdziwymi etykietami a przewidywaniami modelu i dąży do minimalizacji tej różnicy podczas procesu uczenia. Często wykorzystywana w modelach klasyfikacji wieloklasowej.
"""

model=Sequential()
model.add(Dense(128,input_shape=(1000,),activation=activations.relu)) #warstwa wejściowa
model.add(Dense(3, activation=activations.softmax)) #warstwa wyjścia
'''warstwa wyjścia:
3 neurony - 3 kategorie w zbiorze danych,
softmax - wynikiem jest prawdopodobieństwo przynależności do danej kategorii'''
model.compile(loss="categorical_crossentropy", metrics=["accuracy"]) #nie podano optymalizatora, domyślnie: Adam
model.summary()

#uczenie modelu

history=model.fit(train_count.todense(),train_labels_cat,epochs=100,validation_split=0.1)
#validation_split - część zbioru przeznaczona do walidacji

plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
#porównanie zbioru uczącego z walidacyjnym pod względem dokładności - accuracy
plt.legend(["train","val"],loc="upper right")
plt.show()

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
#porównanie zbioru uczącego z walidacyjnym pod względem kosztów - loss
plt.legend(["train","val"],loc="upper right")
plt.show()

model.evaluate(test_count,test_labels_cat)

"""***callback*** - funkcja lub obiekt służąca do wywoływania określonych działań w trakcie uczenia modelu takich jak: wyświetlanie informacji o postępach treningu, zapisywanie stanu modelu w trakcie uczenia, regulacja parametrów uczenia, wczesne zatrzymywanie treningu w przypadku braku poprawy wyników.

"""

#callbacki

es=EarlyStopping(patience=20, verbose=True) #uczenie zatrzymuje się kiedy metryka nie ulega poprawie przez liczbę epok = patience
chp=ModelCheckpoint("best.hdf5", verbose=True, save_best_only=True) #zapisuje najlepsze modele w formacie .hdf5 (nazwa="best.hdf5")

#tworzymy nowy model, żeby nie douczać poprzedniego + dodajemy callbacki,

model=Sequential()
model.add(Dense(128,input_shape=(1000,),activation=activations.relu))
model.add(Dense(3, activation=activations.softmax)) #softmax jest oparta na prawdopodobieństwie, podaje index dla którego p jest największe; 3 bo mamy 3 kategorie
model.compile(loss="categorical_crossentropy", metrics=["accuracy"])
model.summary()
history=model.fit(train_count.todense(),train_labels_cat,epochs=100,validation_split=0.1, callbacks=[es,chp]) #validation_split - część zbioru przeznaczona do walidacji
model.evaluate(test_count,test_labels_cat)

"""####***6. Ocena jakości***"""

model.load_weights("best.hdf5") #załadowanie zapisanych wag ("best.hdf5")
model.evaluate(test_count,test_labels_cat)

model.predict(test_count[:5])
model.predict(test_count[:5]).argmax(1) #zwraca indeks kategorii z największym prawdopodobieństwem przynależności

#macierz pomyłek

confusion_matrix(test_labels_cat.argmax(1),model.predict(test_count).argmax(1))