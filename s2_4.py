# -*- coding: utf-8 -*-
"""S2_4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17dhC1ZvtS831UxwYwBprbm6810LtYR3P
"""

import pandas as pd
import cv2 #przetwarzanie obrazów
import matplotlib.pyplot as plt #wizualizacje
import os #zarządzanie katalogami
!pip install wget #instalowanie pakietu (! -> użycie konsoli)
import wget #pobieranie plików z internetu
import time #zarządzanie czasem
import numpy as np
from sklearn.model_selection import train_test_split #podział zbioru danych
from tensorflow.keras.preprocessing.image import ImageDataGenerator #przetwarzanie obrazów
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras import activations
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

#pobieranie danych z repozytorium

!git clone https://bitbucket.org/pn2020/data-for-data-science.git
!ls
!ls data-for-data-science/

#Tworzenie ramki danych

df=pd.read_csv("data-for-data-science/image_classification_data.csv",sep=";")

#Przegląd danych

df.head()
df["category"].drop_duplicates()
plt.hist(df["category"]) #najwięcej zdjęć z plaży
df.shape

#Tworzenie katalogów do przechowywania zdjęć

os.mkdir("dane") #tworzymy katalog dane
os.mkdir("dane/train")
os.mkdir("dane/test")

wget.download(df["URL"].iloc[0]) #pobiera zdjęcie o pierwszym adresie URL z ramki danych

!ls -la #(lista plików i katalogów) - sprawdzamy czy pobrany plik znajduje sie w strukturze plików

#wyświeltanie zdjęcia

img=cv2.imread("photo-1431576901776-e539bd916ba2")
plt.imshow(img)
plt.show()

df["URL"].iloc[0] #wyświetlamy URL zdjęcia i sprawdzamy czy jest to ten sam plik

'Zdjęcia nie są identyczne - domyślne ustawienia kolorów nie są odpowiednie, należy zdjęie przekonwertować'

#konwertujemy barwy

imp=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
plt.imshow(imp)
plt.show()

#funkcja do pobierania zdjęć

def download_all(df,folder): #wrzucamy do funkcji df i nazwę folderu
  for subfolder in df["category"].drop_duplicates():
    os.mkdir(os.path.join(folder,subfolder)) #dla każdej kategorii w podanym folderze tworzy podfolder o nazwie kategorii
  for i in range(df.shape[0]):
    try:
      #pobiera plik o adresie URL pobranym z ramki danych i nadaje mu jako nazwę godzinę pobrania + losową liczbę czterocyfrową
      output=wget.download(df.iloc[i]["URL"],out=str(round(time.time()))+str(np.random.randint(1000,9999))+".jpg")
      #tworzy ścieżkę do folderu z przydzieloną do pliku kategorią ("category" w ramce danych)
      output2=os.path.join(folder,df.iloc[i]["category"],output)
      os.replace(output, output2)
      img=cv2.imread(output2)
      img=cv2.resize(img,(224,224))
      cv2.imwrite(output2, img)
    except:
        pass

#podział zbioru danych

train, test = train_test_split(df)

#pobieranie zdjęć z wykorzystaniem stworzonej funkcji

download_all(train, "dane/train")
download_all(test, "dane/test")

!ls dane/train/beach

batch_size = 64

train_data_generator = ImageDataGenerator(horizontal_flip=True, rotation_range=30, zoom_range=0.3)
train_data_generator = train_data_generator.flow_from_directory("dane/train", target_size=(224,224), batch_size=batch_size)

test_data_generator = ImageDataGenerator()
test_data_generator = test_data_generator.flow_from_directory("dane/test", target_size=(224,224), batch_size=batch_size)

#Budowa sieci

model = Sequential()
model.add(Conv2D(16, (3,3), input_shape=(224,224,3), activation=activations.relu)) #wyodrębnianie poszczególnych kształtów za pomocą filtrów
model.add(MaxPooling2D()) #redukcja liczby parametrów - wybiera max z warstwy
model.add(Flatten())
model.add(Dense(256, activation=activations.relu))
model.add(Dense(4, activation=activations.softmax))
model.compile(loss="categorical_crossentropy", metrics=["accuracy"])
model.summary()

#callbacki

es = EarlyStopping(patience=2, verbose=True)
rlr = ReduceLROnPlateau(factor=0.1, patience=2, verbose=True) #zmniejszamy współczynnik uczenia, mnożymy go przez factor
chp = ModelCheckpoint("best.hdf5", verbose=True, save_best_only=True)

#Uczenie sieci

history = model.fit(train_data_generator, steps_per_epoch= 13446 // batch_size, epochs=100, validation_data=test_data_generator, validation_steps= 4481 // batch_size, callbacks=[es, chp, rlr])

!ls dane/test/architecture

img = cv2.imread("dane/test/architecture/16793176904210.jpg")
img_temp = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(img_temp)
plt.show()
model.predict(img.reshape(1, 224, 224, 3))

train_data_generator.class_indices

!ls dane/train

from tensorflow.keras.applications.mobilenet import MobileNet
from tensorflow.keras.models import Model

model_MobileNet=MobileNet(include_top=False, pooling="max", weights="imagenet", input_shape=(224,224,3)) #include_top - warstwa górna(ostatnia), będziemy mieli swoją, pooling - jak wyżej, input shape 224*224 piksele, 3 warstwy
x=model_MobileNet.output #nasza warstwa górna
y=Dense(4,activation=activations.softmax)(x)

model=Model(inputs=model_MobileNet.input, outputs=y)
model.summary()

for layer in model.layers[:-1]:
    layer.trenable=False
#ucinamy liczbę warstw bo za dużo parametrów do trenowania
model.summary()

from tensorflow.keras.optimizers import Adam

opt=Adam(learning_rate=0.0001)
model.compile(loss="categorical_crossentropy", metrics=["accuracy"],optimizer=opt) #sieci będą mało modyfikowane ale będą uczone

history = model.fit(train_data_generator, steps_per_epoch= 13446 // batch_size, epochs=3, validation_data=test_data_generator, validation_steps= 4481 // batch_size)

